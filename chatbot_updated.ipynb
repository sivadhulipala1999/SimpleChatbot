{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Chatbot Using PyTorch\n",
    "\n",
    "\n",
    "In this notebook, we shall be building a chatbot based on the original transformer architecture. The data used to train this model has been taken from the movie dialogue corpus which can be seen from <a href=\"https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import pandas as pd \n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "The movie corpus data has to be prepared so that it can be fed to the model we create later on. We first take a look at the data to see what steps need to be taken for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"movie-corpus\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    \"\"\"Print the first n lines of a file\"\"\"\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the sentences are in a JSON file. We process this into a DataFrame as that makes it easier to work with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/movie-corpus/utterances.jsonl\", lines=True)\n",
    "df = (df.groupby(['conversation_id']).agg({'text': lambda x: x.tolist()})).reset_index()\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take a peek at the wonderful DataFrame we just created! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L100001</td>\n",
       "      <td>[then why did you go see mr . koehler in the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L100003</td>\n",
       "      <td>[hi joe ., frank what are you doing here ?, i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L10001</td>\n",
       "      <td>[those guys ain t so tough . i fought plenty o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L100011</td>\n",
       "      <td>[hello ?, frank it s rebecca . i need to see y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L100016</td>\n",
       "      <td>[you killed him . you killed him and i got you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conversation_id                                               text\n",
       "0         L100001  [then why did you go see mr . koehler in the f...\n",
       "1         L100003  [hi joe ., frank what are you doing here ?, i ...\n",
       "2          L10001  [those guys ain t so tough . i fought plenty o...\n",
       "3         L100011  [hello ?, frank it s rebecca . i need to see y...\n",
       "4         L100016  [you killed him . you killed him and i got you..."
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "We now create a vocabulary of words from the sentence pairs created previously. But why do we even need this? Let's analyse this a bit deeper \n",
    "\n",
    "Right now we have a bunch of sentence pairs in our DataFrame which the model cannot directly process. Models can only work with numbers, obviously. So we need to first convert these sentences into numbers. How would we do that? \n",
    "\n",
    "Now, lets say we have a dictionary of words where the id is a number and the value is a word, we could then easily convert each word into an appropriate, meaningful number which can then be used further. This is why we create the vocabulary. \n",
    "\n",
    "To make the vocabulary creation more understandable, we create the \"Voc\" class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"Takes the sentence, splits into words and then adds that to the dictionary\"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "        \n",
    "    def addTokens(self, tokens):\n",
    "        \"\"\"Takes the tokens prepared using the PyTorch library and adds that to the dictionary\"\"\"\n",
    "        for token in tokens:\n",
    "            self.addWord(token)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"Adds each new input word is added to the word-count dictionary, else the word count is increased\"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we cannot create the vocabulary just yet! We have to clean up our sentences to only have relevant words and no gibberish/special tokens. In typical text preprocessing, we do the following \n",
    "<ul>\n",
    "<li>Standardize the case of the string</li>\n",
    "<li>Remove special characters for a simpler processing</li>\n",
    "</ul>\n",
    "\n",
    "We also remove those sentences which are too long, in this case longer than the parameter MAX_LENGTH, so as to avoid too much resource consumption in the model training phase. So any sentence longer than 10 words will be removed from the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 83097 sentence pairs\n",
      "Trimmed to 83097 sentence pairs\n",
      "\n",
      "Texts:\n",
      "conversation_id                                  L100050\n",
      "text               [can i go ?, you get his statement ?]\n",
      "Name: 10, dtype: object\n",
      "conversation_id                        L100052\n",
      "text               [yeah ., then you can go .]\n",
      "Name: 11, dtype: object\n",
      "conversation_id                                    L100071\n",
      "text               [are you seeing betty tonight ?, nah .]\n",
      "Name: 14, dtype: object\n",
      "conversation_id                                           L10012\n",
      "text               [well well . mrs . brigman ., not for long .]\n",
      "Name: 27, dtype: object\n",
      "conversation_id                                              L100133\n",
      "text               [it s a beautiful picture of her ., why are th...\n",
      "Name: 32, dtype: object\n",
      "conversation_id                                              L100157\n",
      "text               [carolyn you want these candlesticks ?, no . y...\n",
      "Name: 38, dtype: object\n",
      "conversation_id                                               L10026\n",
      "text               [i think you were worried about me ., that mus...\n",
      "Name: 60, dtype: object\n",
      "conversation_id                                       L100271\n",
      "text               [are you supposed to be in iowa ?, yeah .]\n",
      "Name: 62, dtype: object\n",
      "conversation_id                          L100287\n",
      "text               [pretty country ., hmm mmm .]\n",
      "Name: 66, dtype: object\n",
      "conversation_id                                       L100320\n",
      "text               [oh there you are ., oh ! you caught me .]\n",
      "Name: 73, dtype: object\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalizeTexts(texts):\n",
    "    newTexts = []\n",
    "    for s in texts:\n",
    "        s = s.lower().strip()\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "        newTexts.append(s)\n",
    "    return newTexts\n",
    "\n",
    "# Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterCondition(texts):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    if len(texts) > 2:\n",
    "        considerText = False\n",
    "    else: \n",
    "        considerText = True\n",
    "        for sentence in texts:\n",
    "            considerText = considerText and (len(sentence.split(\" \")) < MAX_LENGTH)\n",
    "    return considerText\n",
    "\n",
    "def filterData(pairs):\n",
    "    return [texts for texts in pairs if filterCondition(texts)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def prepareData(df):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    df[\"text\"] = df[\"text\"].apply(normalizeTexts)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(df)))\n",
    "    new_df = df.loc[df[\"text\"].apply(filterCondition)]\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(df)))\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "input_data = prepareData(df)\n",
    "# Print some pairs to validate\n",
    "print(\"\\nTexts:\")\n",
    "for idx in range(10):\n",
    "    print(input_data.iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could start making the vocabulary right away no problem! However, I want to simplify the DataFrame a bit more so that I have an easier time dealing with it. So I split the sentence pairs into questions, answers and then proceed further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_28652\\1859300778.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_data.loc[:, \"questions\"] = input_data.loc[:, \"text\"].apply(lambda l: l[0])\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_28652\\1859300778.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_data.loc[:, \"answers\"] = input_data.loc[:, \"text\"].apply(lambda l: l[1])\n"
     ]
    }
   ],
   "source": [
    "input_data.loc[:, \"questions\"] = input_data.loc[:, \"text\"].apply(lambda l: l[0])\n",
    "input_data.loc[:, \"answers\"] = input_data.loc[:, \"text\"].apply(lambda l: l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally comes the vocabulary part! We now split the sentence into \"tokens\" using a tokenizer. Each \"token\" may not be the same as a word and we use a tokenizer for this part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_28652\\4226510538.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_data.loc[:, \"question_tokens\"] = input_data.loc[:, \"questions\"].apply(tokenizer)\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_28652\\4226510538.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_data.loc[:, \"answer_tokens\"] = input_data.loc[:, \"answers\"].apply(tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8195"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "input_data.loc[:, \"question_tokens\"] = input_data.loc[:, \"questions\"].apply(tokenizer)\n",
    "input_data.loc[:, \"answer_tokens\"] = input_data.loc[:, \"answers\"].apply(tokenizer)\n",
    "\n",
    "\n",
    "voc = Voc(corpus_name)\n",
    "\n",
    "input_data.loc[:, \"question_tokens\"].apply(voc.addTokens)\n",
    "input_data.loc[:, \"answer_tokens\"].apply(voc.addTokens)\n",
    "\n",
    "voc.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to converting the sentences into number sequences, with each number being an ID for the token in the vocabulary. Since the model needs to know when a sequence starts and ends, we add special SOS_token and EOS_token. \n",
    "\n",
    "SOS = Start Of Sequence, EOS = End Of Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = input_data.loc[:, \"question_tokens\"].apply(\n",
    "    lambda l: [SOS_token] + [voc.word2index[ele] for ele in l] + [EOS_token])\n",
    "output_tokens = input_data.loc[:, \"answer_tokens\"].apply(\n",
    "    lambda l: [SOS_token] + [voc.word2index[ele] for ele in l] + [EOS_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                                     [can, i, go, ?]\n",
       "11                                           [yeah, .]\n",
       "14               [are, you, seeing, betty, tonight, ?]\n",
       "27                 [well, well, ., mrs, ., brigman, .]\n",
       "32          [it, s, a, beautiful, picture, of, her, .]\n",
       "                             ...                      \n",
       "83065                               [no, questions, .]\n",
       "83076                      [objection, your, honor, .]\n",
       "83086                                     [i, know, .]\n",
       "83089    [let, me, go, !, godammit, frank, let, go, !]\n",
       "83092             [you, did, a, good, job, charlie, .]\n",
       "Name: question_tokens, Length: 12043, dtype: object"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.loc[:, \"question_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12837, 13225)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check to see if there's any sentences longer than 10. If there's not too many, we just drop these ones \n",
    "input_tokens.apply(lambda l: len(l) if len(l) > 10 else 0).sum(), output_tokens.apply(lambda l: len(l) if len(l) > 10 else 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the sequences we created are of different lengths. The Transformer architecture takes a fixed length sequence as input by default. How do we solve this then? With Padding of course! \n",
    "\n",
    "We add a bunch of \"PAD_tokens\" at the end of the sequence to make all the sequences of the same length. They mean absolutely nothing to the model and we make sure of it by adding a mask in the architecture, more on this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: [tensor([1, 3, 4, 5, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 7, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([ 1,  9, 10, 11, 12, 13,  6,  2,  0,  0,  0,  0,  0,  0,  0]), tensor([ 1, 14, 14,  8, 15,  8, 16,  8,  2,  0,  0,  0,  0,  0,  0]), tensor([ 1, 17, 18, 19, 20, 21, 22, 23,  8,  2,  0,  0,  0,  0,  0])]\n",
      "lengths: tensor([15, 15, 15, 15, 15])\n",
      "target_variable: [tensor([   1,   10,  188,  319, 2672,    6,    2,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0]), tensor([  1, 358,  10,   3,   5,   8,   2,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0]), tensor([   1, 2194,    8,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0]), tensor([  1, 111, 144, 654,   8,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0]), tensor([   1,  224,    9,   41,  855, 3559,  108,    6,    2,    0,    0,    0,\n",
      "           0,    0,    0])]\n",
      "mask: tensor([True, True, True, True, True])\n",
      "max_target_len: 15\n"
     ]
    }
   ],
   "source": [
    "PAD_LENGTH = 15\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return l + [PAD_token] * (PAD_LENGTH - len(l))\n",
    "\n",
    "def padMask(l, value=PAD_token):\n",
    "    m = []\n",
    "    for ele in l: \n",
    "        if ele == value:\n",
    "            m.append(0)\n",
    "        else: \n",
    "            m.append(1)\n",
    "    return m  \n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    input_tokens = l.apply(zeroPadding)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in input_tokens])\n",
    "    padVar = [torch.LongTensor(padElement) for padElement in input_tokens]\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    output_tokens = l.apply(zeroPadding)\n",
    "    max_target_len = max([len(indexes) for indexes in output_tokens])\n",
    "    padmask = padMask(output_tokens.apply(lambda l: len(l)))\n",
    "    padmask = torch.BoolTensor(padmask)\n",
    "    padVar = [torch.LongTensor(padElement) for padElement in output_tokens]\n",
    "    return padVar, padmask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, input_tokens, output_tokens, num_of_pairs):\n",
    "    input_tokens_list = input_tokens.iloc[:num_of_pairs]\n",
    "    output_tokens_list = output_tokens.iloc[:num_of_pairs]\n",
    "    inp, lengths = inputVar(input_tokens_list, voc)\n",
    "    output, mask, max_target_len = outputVar(output_tokens_list, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, input_tokens, output_tokens, small_batch_size)\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Modelling time! \n",
    "\n",
    "We shall be using the original Transformer architecture from the \"Attention is all you need\" paper. The minor changes made to the architecture in our case are as follows, \n",
    "\n",
    "<ul>\n",
    "<li>Query, Key and Value vectors are not being prepared as per the \"learned weight matrices\". Instead we simply pass the input as the query, key and values for easier computation</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "If you do not know the architecture, I recommend reading the handout file which I included in the repo. Of course, it just glances over the architecture but I strongly believe that it gives a good overview and you can dive deeper on the internet when necessary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following blocks to create: \n",
    "- Multi Head Attention Block - which implements the attention formula \n",
    "- Encoder Block - containing all the \"Add & Norm\", Attention layers needed \n",
    "- Decoder Block - again containing all the necessary layers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned a \"mask\" before the architecture, if you were paying attention (ba dum tss!). This is where it comes into play! \n",
    "\n",
    "The following code combines all the blocks we have built so far into a final Transformer class. In this class, we see a function called \"generate_masks\" that is return a src_mask and a tgt_mask. \n",
    "\n",
    "The src_mask checks which tokens are PAD_tokens and marks those positions and purposefully give them low attention scores (check the MultiHeadAttention block for this). The tgt_mask masks all the future tokens for each given token so that the Decoder cannot cheat and should actually learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeek_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeek_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just create the Transformer with a set of chosen Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = voc.num_words\n",
    "d_model = 512\n",
    "tgt_vocab_size = voc.num_words # source and target vocab belong to the same Voc object \n",
    "num_heads = 8\n",
    "num_layers = 6 # number of encoder, decoder stacks\n",
    "d_ff = 2048 # dimensionality of the hidden layer for the feed forward neural network \n",
    "max_seq_length = PAD_LENGTH # same as the original input because you add the input tensor to the Positional Encoding tensor   \n",
    "dropout = 0.1 # dropout probability. Initial value 0.1 \n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time! \n",
    "\n",
    "We can take a larger batch size but for the purpose of this notebook, I considered 64 as the batch size per epoch and 100 epochs in total since my system can only handle so much training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 9.330514907836914\n",
      "Epoch: 2, Loss: 7.404484272003174\n",
      "Epoch: 3, Loss: 6.7136921882629395\n",
      "Epoch: 4, Loss: 6.480833530426025\n",
      "Epoch: 5, Loss: 6.34003210067749\n",
      "Epoch: 6, Loss: 6.228860378265381\n",
      "Epoch: 7, Loss: 6.134629726409912\n",
      "Epoch: 8, Loss: 6.040375232696533\n",
      "Epoch: 9, Loss: 5.932708263397217\n",
      "Epoch: 10, Loss: 5.823095321655273\n",
      "Epoch: 11, Loss: 5.6992926597595215\n",
      "Epoch: 12, Loss: 5.568178653717041\n",
      "Epoch: 13, Loss: 5.431950092315674\n",
      "Epoch: 14, Loss: 5.326467514038086\n",
      "Epoch: 15, Loss: 5.175271987915039\n",
      "Epoch: 16, Loss: 5.047525882720947\n",
      "Epoch: 17, Loss: 4.9178338050842285\n",
      "Epoch: 18, Loss: 4.790621757507324\n",
      "Epoch: 19, Loss: 4.666529178619385\n",
      "Epoch: 20, Loss: 4.5362935066223145\n",
      "Epoch: 21, Loss: 4.403532981872559\n",
      "Epoch: 22, Loss: 4.261481761932373\n",
      "Epoch: 23, Loss: 4.125705242156982\n",
      "Epoch: 24, Loss: 3.9655356407165527\n",
      "Epoch: 25, Loss: 3.8199057579040527\n",
      "Epoch: 26, Loss: 3.683149576187134\n",
      "Epoch: 27, Loss: 3.5613436698913574\n",
      "Epoch: 28, Loss: 3.41880202293396\n",
      "Epoch: 29, Loss: 3.31535005569458\n",
      "Epoch: 30, Loss: 3.210411310195923\n",
      "Epoch: 31, Loss: 3.0362486839294434\n",
      "Epoch: 32, Loss: 2.902207374572754\n",
      "Epoch: 33, Loss: 2.827958345413208\n",
      "Epoch: 34, Loss: 2.6561055183410645\n",
      "Epoch: 35, Loss: 2.593451976776123\n",
      "Epoch: 36, Loss: 2.463939905166626\n",
      "Epoch: 37, Loss: 2.3365955352783203\n",
      "Epoch: 38, Loss: 2.2390708923339844\n",
      "Epoch: 39, Loss: 2.1192703247070312\n",
      "Epoch: 40, Loss: 2.022584915161133\n",
      "Epoch: 41, Loss: 1.906156301498413\n",
      "Epoch: 42, Loss: 1.821306586265564\n",
      "Epoch: 43, Loss: 1.7258424758911133\n",
      "Epoch: 44, Loss: 1.6506110429763794\n",
      "Epoch: 45, Loss: 1.5518141984939575\n",
      "Epoch: 46, Loss: 1.4690139293670654\n",
      "Epoch: 47, Loss: 1.4012688398361206\n",
      "Epoch: 48, Loss: 1.3206596374511719\n",
      "Epoch: 49, Loss: 1.2352269887924194\n",
      "Epoch: 50, Loss: 1.1705070734024048\n",
      "Epoch: 51, Loss: 1.1084849834442139\n",
      "Epoch: 52, Loss: 1.0404834747314453\n",
      "Epoch: 53, Loss: 0.9798188209533691\n",
      "Epoch: 54, Loss: 0.9243112206459045\n",
      "Epoch: 55, Loss: 0.8482791781425476\n",
      "Epoch: 56, Loss: 0.8061702847480774\n",
      "Epoch: 57, Loss: 0.756028950214386\n",
      "Epoch: 58, Loss: 0.7210718989372253\n",
      "Epoch: 59, Loss: 0.6629002094268799\n",
      "Epoch: 60, Loss: 0.5992602109909058\n",
      "Epoch: 61, Loss: 0.5916450023651123\n",
      "Epoch: 62, Loss: 0.5457273721694946\n",
      "Epoch: 63, Loss: 0.4955165684223175\n",
      "Epoch: 64, Loss: 0.4749334752559662\n",
      "Epoch: 65, Loss: 0.43120044469833374\n",
      "Epoch: 66, Loss: 0.40601322054862976\n",
      "Epoch: 67, Loss: 0.3811917304992676\n",
      "Epoch: 68, Loss: 0.3485863506793976\n",
      "Epoch: 69, Loss: 0.3320753872394562\n",
      "Epoch: 70, Loss: 0.30537980794906616\n",
      "Epoch: 71, Loss: 0.29214200377464294\n",
      "Epoch: 72, Loss: 0.27415701746940613\n",
      "Epoch: 73, Loss: 0.27028486132621765\n",
      "Epoch: 74, Loss: 0.23788179457187653\n",
      "Epoch: 75, Loss: 0.2274150848388672\n",
      "Epoch: 76, Loss: 0.20772600173950195\n",
      "Epoch: 77, Loss: 0.19812358915805817\n",
      "Epoch: 78, Loss: 0.18299317359924316\n",
      "Epoch: 79, Loss: 0.18069766461849213\n",
      "Epoch: 80, Loss: 0.16697587072849274\n",
      "Epoch: 81, Loss: 0.16013485193252563\n",
      "Epoch: 82, Loss: 0.14692151546478271\n",
      "Epoch: 83, Loss: 0.13896134495735168\n",
      "Epoch: 84, Loss: 0.1381267011165619\n",
      "Epoch: 85, Loss: 0.13510790467262268\n",
      "Epoch: 86, Loss: 0.1236858069896698\n",
      "Epoch: 87, Loss: 0.11810114234685898\n",
      "Epoch: 88, Loss: 0.12581674754619598\n",
      "Epoch: 89, Loss: 0.10769986361265182\n",
      "Epoch: 90, Loss: 0.12035905569791794\n",
      "Epoch: 91, Loss: 0.10292728990316391\n",
      "Epoch: 92, Loss: 0.11568347364664078\n",
      "Epoch: 93, Loss: 0.0963243916630745\n",
      "Epoch: 94, Loss: 0.09962236136198044\n",
      "Epoch: 95, Loss: 0.08914364129304886\n",
      "Epoch: 96, Loss: 0.08989359438419342\n",
      "Epoch: 97, Loss: 0.08415448665618896\n",
      "Epoch: 98, Loss: 0.08002878725528717\n",
      "Epoch: 99, Loss: 0.08327942341566086\n",
      "Epoch: 100, Loss: 0.07625490427017212\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "batch_size = 64 # each batch is a pair of sentences \n",
    "n_batches = 1\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # training data preparation \n",
    "    training_batch = batch2TrainData(voc, input_tokens, output_tokens, batch_size)\n",
    "    input_variable, _, target_variable, _, _ = training_batch\n",
    "\n",
    "    # input_list is a list of tensors right now - [(tensor), (tensor), .... n_batches times]\n",
    "    temp = [(input_var, target_var) for input_var, target_var in zip(input_variable, target_variable) \n",
    "                                         if ((len(input_var) == PAD_LENGTH) and (len(target_var) == PAD_LENGTH))]\n",
    "    res = list(zip(*temp))\n",
    "    input_variable = list(res[0])\n",
    "    target_variable = list(res[1])\n",
    "\n",
    "    input_variable = torch.stack(input_variable, dim=0)\n",
    "    target_variable = torch.stack(target_variable, dim=0)\n",
    "\n",
    "    output = transformer(input_variable, target_variable[:, :-1]) # shifting decoder input by 1 token \n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), target_variable[:, 1:].contiguous().view(-1)) \n",
    "    # exclude the first token for calculating the loss \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment of truth! We now check how it behaves on an input\n",
    "\n",
    "One thing to note here that really confused me at first is how to actually perform inference with the model created. \n",
    "\n",
    "As you know from the architecture, Decoder parses the tokens one-by-one, meaning that it takes one token and generates the next one. But how does it do that? You might have seen that the loss we chose was the CrossEntropyLoss, used during multi-class classification problems and considers probability as the output.  \n",
    "\n",
    "This means that we get a bunch of probabilities as the output. If we check which index has the highest probability and note that, we can map it to the original word in the vocabulary and then get the final output! In hindsight, this is quite simple but I had difficulty wrapping my head around it. Hope this helps people! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  Hi\n",
      "chatbot: what ? oh yeah .\n"
     ]
    }
   ],
   "source": [
    "def sentenceFromIndexes(voc, indexes):\n",
    "    sent_tokens = [voc.index2word[idx.item()] for idx in indexes[0] if idx.item() not in [0, 1, 2]]\n",
    "    return \"chatbot: \" + \" \".join(sent_tokens) \n",
    "\n",
    "def chat(input_stmt):\n",
    "    print(\"> \", input_stmt)\n",
    "\n",
    "    # prepare the input \n",
    "    input_tokens_inference = tokenizer(input_stmt)\n",
    "    input_tokens_inference = [SOS_token] + [voc.word2index[ele] for ele in input_tokens_inference] + [EOS_token]\n",
    "    \n",
    "    input_variable, _ = inputVar(pd.Series([input_tokens_inference]), voc)\n",
    "    # decoder_input = torch.zeros(MAX_LENGTH - 1).unsqueeze(0)\n",
    "    # decoder_input[0, 0] = SOS_token # make the first token of the decoder as Start-of-Sequence -- SOS \n",
    "    decoder_input = torch.tensor([SOS_token]).unsqueeze(0)\n",
    "\n",
    "    # we can pass this input to the encoder, but what about the decoder? \n",
    "    for i in range(PAD_LENGTH):\n",
    "        prediction = transformer(input_variable[0].unsqueeze(0).to(torch.long), decoder_input.to(torch.long))\n",
    "        prediction = prediction[:, -1:]\n",
    "        predicted_id = torch.argmax(prediction, axis=2).to(torch.long) # find the token ID which has the highest probability as per the transformer\n",
    "\n",
    "        if predicted_id == EOS_token:\n",
    "            break  \n",
    "\n",
    "        decoder_input = torch.concat([decoder_input, predicted_id], axis=-1)\n",
    "    return sentenceFromIndexes(voc, decoder_input.to(torch.long))\n",
    "\n",
    "print(chat(\"Hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marks the end of the notebook! I hope you learned something from this. The main goal was to showcase my understanding of the architecture and to clear any misunderstandings I might have had. \n",
    "\n",
    "Finally a chatbot created from scratch! One item off my bucket list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some side notes\n",
    "<ul>\n",
    "<li>Embedding layer is a lookup table which takes as input the word indexes and outputs the corresponding embeddings for these indexes</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
